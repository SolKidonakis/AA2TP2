{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOqWz8bx8ODa"
   },
   "source": [
    "**TRABAJO PRACTICO N°2: APRENDIZAJE AUTOMATICO 2**\n",
    "\n",
    "**MIEMBROS: SOL KIDONAKIS Y AGUSTIN ARENAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWsjFsbi8Vlh"
   },
   "source": [
    "**LIBRERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "jzeutgwovQRD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gL8QBtNowT4g"
   },
   "outputs": [],
   "source": [
    "# Configurar la GPU si está disponible\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVv9U0R58YA9"
   },
   "source": [
    "**CARGA DEL DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f81bTkDzuacn",
    "outputId": "62b2bbaa-8595-4e9a-ac21-a1a9089e3aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Longitud del texto: 1115394 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Descargar el dataset\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    ")\n",
    "\n",
    "# Leer el contenido\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f\"Longitud del texto: {len(text)} caracteres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "co-2VDbcuxBw",
    "outputId": "898c69f6-3489-48fe-9105-843fd50c3fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las primeras líneas\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOYJvYv_u7IO",
    "outputId": "8ed36f61-eef2-4bb2-8a3f-5243e9ce4122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 caracteres únicos en el texto\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} caracteres únicos en el texto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6Ecfz7HwdC8"
   },
   "source": [
    "**PREPROCESAMIENTO PARA EL MODELO CARACTER A CARACTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DBKgrVzwpGh"
   },
   "source": [
    "Para el modelo carácter a carácter:\n",
    "-Crear un vocabulario de caracteres únicos.\n",
    "-Mapear caracteres a índices y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Mapear caracteres a índices y viceversa\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "# Convertir texto a índices\n",
    "text_as_int = [char2idx[char] for char in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRA1-0m3wuci"
   },
   "source": [
    "Generar secuencias de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "NIx4Cvr6wvE2"
   },
   "outputs": [],
   "source": [
    "# Longitud de secuencias para el modelo\n",
    "SEQ_LENGTH = 100  \n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Dividir el texto en secuencias\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "# Crear pares entrada-objetivo\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-qsgfouCVf"
   },
   "source": [
    "**DEFINICION MODELO CARACTER A CARACTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "ABVeW_QcuGie"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_8 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │        \u001b[38;5;34m66,625\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,021,569</span> (15.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,021,569\u001b[0m (15.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    inputs = tf.keras.Input(batch_shape=(batch_size, None))  # Definir las entradas explícitamente\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "    x = tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')(x)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Parámetros del modelo\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# Crear el modelo\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsEQpV8Nxepr"
   },
   "source": [
    "**ENTRENAMIENTO DEL MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 990ms/step - loss: 3.1132\n",
      "Epoch 2/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 1.9212\n",
      "Epoch 3/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 1.6491\n",
      "Epoch 4/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 1.5064\n",
      "Epoch 5/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 1s/step - loss: 1.4226\n",
      "Epoch 6/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 1.3692\n",
      "Epoch 7/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - loss: 1.3260\n",
      "Epoch 8/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 1s/step - loss: 1.2898\n",
      "Epoch 9/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - loss: 1.2525\n",
      "Epoch 10/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1s/step - loss: 1.2208\n"
     ]
    }
   ],
   "source": [
    "# Función de pérdida\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Entrenar el modelo\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MlbGaXHyigB"
   },
   "source": [
    "**GENERACION DE TEXTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo con batch_size=1 para inferencia\n",
    "gen_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "gen_model.set_weights(model.get_weights())  # Cargar los pesos entrenados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_char(model, seed_text, num_generate, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Genera texto carácter a carácter usando un modelo entrenado con batch_size=1.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado carácter a carácter (batch_size=1).\n",
    "        seed_text: Texto inicial para comenzar la generación.\n",
    "        num_generate: Número de caracteres a generar.\n",
    "        temperature: Controla la creatividad de las predicciones (más bajo = más predecible).\n",
    "    \n",
    "    Returns:\n",
    "        Texto generado.\n",
    "    \"\"\"\n",
    "    # Convertir texto semilla a índices\n",
    "    input_eval = [char2idx[char] for char in seed_text]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)  # Agregar dimensión de lote\n",
    "\n",
    "    # Lista para texto generado\n",
    "    text_generated = []\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        # Generar predicciones\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # Quitar dimensión del lote y ajustar por temperatura\n",
    "        predictions = tf.squeeze(predictions, 0)  # Ahora será (sequence_length, vocab_size)\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # Seleccionar el siguiente carácter\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Agregar carácter generado al texto\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "        # Actualizar la entrada para la próxima iteración\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    # Combinar el texto generado con el inicial\n",
    "    return seed_text + ''.join(text_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros\n",
    "temperaturas = [0.5, 1.0, 1.5]  # Diferentes temperaturas para probar\n",
    "longitudes = [50, 100, 200]  # Diferentes longitudes de texto a generar\n",
    "seed_text = \"To be\"  # Texto inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperatura: 0.5 ---\n",
      "\n",
      "--- Longitud: 50 caracteres ---\n",
      "To be the first of all the house of York,\n",
      "That would no\n",
      "\n",
      "\n",
      "--- Longitud: 100 caracteres ---\n",
      "To be so strife: he has been done,\n",
      "That had show it confess,\n",
      "To see the time to the last,\n",
      "And not not sti\n",
      "\n",
      "\n",
      "--- Longitud: 200 caracteres ---\n",
      "To begin to send thy lands\n",
      "That we have sometime consul should be\n",
      "the storm and seven such as the son:\n",
      "The garden of the seasons of my son,\n",
      "I'll vent that villain and the beggar of our grave?\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Temperatura: 1.0 ---\n",
      "\n",
      "--- Longitud: 50 caracteres ---\n",
      "To beat and look chosen me\n",
      "To figure our way, as they a\n",
      "\n",
      "\n",
      "--- Longitud: 100 caracteres ---\n",
      "To beat\n",
      "Master resident, and chosen till Richmond,\n",
      "imsolsture your leave should be taught,\n",
      "And there a me\n",
      "\n",
      "\n",
      "--- Longitud: 200 caracteres ---\n",
      "To bear well affection, and, and lacks,\n",
      "I should not have no lenitemanted sled.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why?\n",
      "\n",
      "RUTHOMAS:\n",
      "Your prison when they sleep?\n",
      "\n",
      "MARCIUS:\n",
      "Thou nobless, marroy, PETESTRLIA:\n",
      "Feward in the household\n",
      "\n",
      "\n",
      "\n",
      "--- Temperatura: 1.5 ---\n",
      "\n",
      "--- Longitud: 50 caracteres ---\n",
      "To bed,\n",
      "That which sitsld pluck, hiz. Good, boy, ELY:\n",
      "B\n",
      "\n",
      "\n",
      "--- Longitud: 100 caracteres ---\n",
      "To betime.\n",
      "\n",
      "LUCENTIO:\n",
      "\n",
      "Would\n",
      "And give me accemant, Part us to\n",
      "unto you, Romeo;; who answer more,\n",
      "Staye my\n",
      "\n",
      "\n",
      "--- Longitud: 200 caracteres ---\n",
      "To bear with my fast.\n",
      "\n",
      "HENTENESBRISH:\n",
      "OBpoisleyanting on't,; I nevay not\n",
      "him,\n",
      "As thick mine any canqeisf, agree\n",
      "Should before things sometime condect himself\n",
      "Atthangle an argy tocet, Predia't is,\n",
      "Are spend\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generar texto para cada combinación\n",
    "for temp in temperaturas:\n",
    "    print(f\"\\n--- Temperatura: {temp} ---\\n\")\n",
    "    for length in longitudes:\n",
    "        print(f\"--- Longitud: {length} caracteres ---\")\n",
    "        generated_text = generate_text_char(\n",
    "            model=gen_model,  # Modelo con batch_size=1\n",
    "            seed_text=seed_text,\n",
    "            num_generate=length,\n",
    "            temperature=temp\n",
    "        )\n",
    "        print(generated_text)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLQZwhGG_udZ"
   },
   "source": [
    "**PREPROCESAMIENTO PARA EL MODELO PALABRA A PALABRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhl_GnoO_xPZ",
    "outputId": "a7fc9987-c8d0-4006-d8d9-f8c6e8c9311d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12633\n"
     ]
    }
   ],
   "source": [
    "# Tokenización a nivel de palabras\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])  # Construye el vocabulario a partir del texto\n",
    "\n",
    "# Convertir el texto en una secuencia de índices de palabras\n",
    "word_indices = tokenizer.texts_to_sequences([text])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 para incluir el índice 0\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ovj8iraT3V9F"
   },
   "outputs": [],
   "source": [
    "# Crear secuencias de entrenamiento\n",
    "seq_length = 10\n",
    "sequences = []\n",
    "\n",
    "for i in range(seq_length, len(word_indices)):\n",
    "    seq = word_indices[i - seq_length:i + 1]  # secuencia de entrada + palabra objetivo\n",
    "    sequences.append(seq)\n",
    "\n",
    "# Convertir a TensorFlow Dataset\n",
    "sequences = tf.constant(sequences)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "\n",
    "# Dividir en input y target\n",
    "def split_input_target(seq):\n",
    "    input_text = seq[:-1]  # Todas menos la última palabra\n",
    "    target_text = seq[-1]  # Última palabra\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "# Mezclar y agrupar en batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP1_80duyGUw"
   },
   "source": [
    "**DEFINICION DEL MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XewuWIfbyBbk"
   },
   "outputs": [],
   "source": [
    "# Parámetros del modelo\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# Construcción del modelo\n",
    "class WordModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=False, return_state=True)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "       x = self.embedding(inputs, training=training)\n",
    "       if states is None:\n",
    "         batch_size = tf.shape(inputs)[0]\n",
    "         states = tf.zeros((batch_size, self.gru.units))\n",
    "       x, states = self.gru(x, initial_state=states, training=training)\n",
    "       x = self.dense(x, training=training)  # Salida directa\n",
    "       if return_state:\n",
    "          return x, states\n",
    "       else:\n",
    "          return x\n",
    "\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = WordModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKKaltKryOC8",
    "outputId": "25c2c4cf-376a-4c40-ca1e-7959c8f8ed2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1036s\u001b[0m 324ms/step - loss: 6.6276\n",
      "Epoch 2/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 249ms/step - loss: 5.5502\n",
      "Epoch 3/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 232ms/step - loss: 4.6793\n",
      "Epoch 4/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 228ms/step - loss: 3.4249\n",
      "Epoch 5/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 227ms/step - loss: 2.3503\n",
      "Epoch 6/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 227ms/step - loss: 1.6412\n",
      "Epoch 7/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 227ms/step - loss: 1.2025\n",
      "Epoch 8/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 223ms/step - loss: 0.9497\n",
      "Epoch 9/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 222ms/step - loss: 0.8287\n",
      "Epoch 10/10\n",
      "\u001b[1m3188/3188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 222ms/step - loss: 0.7694\n"
     ]
    }
   ],
   "source": [
    "# Función de pérdida\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Callback para checkpoints\n",
    "checkpoint_dir = './word_training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"{checkpoint_dir}/ckpt_{{epoch:02d}}.weights.h5\",\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0awPQg_S33AF"
   },
   "source": [
    "**GENERACION DE TEXTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hCFbMEjayRK7"
   },
   "outputs": [],
   "source": [
    "def generate_text_word(model, seed_text, next_words, seq_length, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        # Convertir texto semilla en secuencia de índices\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        \n",
    "        # Asegurar que la secuencia tenga la longitud esperada\n",
    "        token_list = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [token_list], maxlen=seq_length, padding='pre'\n",
    "        )\n",
    "        \n",
    "        # Obtener predicciones\n",
    "        predictions = model.predict(token_list, verbose=0)\n",
    "        predictions = predictions / temperature  # Ajustar por temperatura\n",
    "        \n",
    "        # Seleccionar la palabra más probable\n",
    "        predicted_word_index = np.argmax(predictions, axis=-1)[0]\n",
    "        \n",
    "        # Obtener la palabra correspondiente\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, \"<UNK>\")\n",
    "        \n",
    "        # Agregar palabra generada al texto semilla\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de parámetros\n",
    "temperaturas = [0.5, 1.0, 1.5]  # Valores para evaluar\n",
    "longitudes = [5, 10, 20]  # Longitudes de secuencia a probar\n",
    "seed_text = \"To be\"  # Texto inicial para todas las pruebas\n",
    "next_words = 50  # Número de palabras a generar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Longitud de secuencia: 5 ---\n",
      "\n",
      "Temperatura: 0.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o o o prey sometime he's thy thought ready now they perceive best mercutio and fly alas witness sir now sits son and saving long prithee candle other they intend here true but they persuade noble service o they intend grace doing vantage now good marcius now be prompt\n",
      "\n",
      "\n",
      "Temperatura: 1.0\n",
      "Texto generado:\n",
      "\n",
      "To be barren o o o prey sometime he's thy thought ready now they perceive best mercutio and fly alas witness sir now sits son and saving long prithee candle other they intend here true but they persuade noble service o they intend grace doing vantage now good marcius now be prompt\n",
      "\n",
      "\n",
      "Temperatura: 1.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o o o prey sometime he's thy thought ready now they perceive best mercutio and fly alas witness sir now sits son and saving long prithee candle other they intend here true but they persuade noble service o they intend grace doing vantage now good marcius now be prompt\n",
      "\n",
      "\n",
      "\n",
      "--- Longitud de secuencia: 10 ---\n",
      "\n",
      "Temperatura: 0.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o now shall be done sometime which grave tush were thy right power but now prompt defend like power see vile that filling and did stand take anger still part forbid to anger second noble power lack gaoler and sometime right officer choler and seize stones true if true\n",
      "\n",
      "\n",
      "Temperatura: 1.0\n",
      "Texto generado:\n",
      "\n",
      "To be barren o now shall be done sometime which grave tush were thy right power but now prompt defend like power see vile that filling and did stand take anger still part forbid to anger second noble power lack gaoler and sometime right officer choler and seize stones true if true\n",
      "\n",
      "\n",
      "Temperatura: 1.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o now shall be done sometime which grave tush were thy right power but now prompt defend like power see vile that filling and did stand take anger still part forbid to anger second noble power lack gaoler and sometime right officer choler and seize stones true if true\n",
      "\n",
      "\n",
      "\n",
      "--- Longitud de secuencia: 20 ---\n",
      "\n",
      "Temperatura: 0.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o which now he's he's he's dead and saving long o anger be ope when now was fear barren now part other and be noble nobly messenger and give quench request mother faith be noble friend emilia gaoler and lie which they say so anger thanks neighbour sweet first\n",
      "\n",
      "\n",
      "Temperatura: 1.0\n",
      "Texto generado:\n",
      "\n",
      "To be barren o which now he's he's he's dead and saving long o anger be ope when now was fear barren now part other and be noble nobly messenger and give quench request mother faith be noble friend emilia gaoler and lie which they say so anger thanks neighbour sweet first\n",
      "\n",
      "\n",
      "Temperatura: 1.5\n",
      "Texto generado:\n",
      "\n",
      "To be barren o which now he's he's he's dead and saving long o anger be ope when now was fear barren now part other and be noble nobly messenger and give quench request mother faith be noble friend emilia gaoler and lie which they say so anger thanks neighbour sweet first\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_length in longitudes:\n",
    "    print(f\"\\n--- Longitud de secuencia: {seq_length} ---\\n\")\n",
    "    \n",
    "    # Ajustar longitud de secuencia en el preprocesamiento\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    word_indices = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequences = []\n",
    "    for i in range(seq_length, len(word_indices)):\n",
    "        seq = word_indices[i - seq_length:i + 1]\n",
    "        sequences.append(seq)\n",
    "\n",
    "    sequences = tf.constant(sequences)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "    dataset = dataset.map(split_input_target)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    for temp in temperaturas:\n",
    "        print(f\"Temperatura: {temp}\")\n",
    "        print(\"Texto generado:\\n\")\n",
    "        generated_text = generate_text_word(\n",
    "            model=model,\n",
    "            seed_text=seed_text,\n",
    "            next_words=next_words,\n",
    "            seq_length=seq_length,\n",
    "            temperature=temp\n",
    "        )\n",
    "        print(generated_text)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impacto de Temperatura\n",
    "Temperatura = 0.5:\n",
    "\n",
    "Produce texto más conservador, con menor variación creativa.\n",
    "Tiende a repetir frases o patrones similares.\n",
    "\n",
    "Temperatura = 1.0:\n",
    "\n",
    "Proporciona un equilibrio entre coherencia y creatividad.\n",
    "Genera frases que tienen un flujo más natural y un mejor balance entre repetición y novedad.\n",
    "\n",
    "Desventaja: Puede incluir frases menos conectadas.\n",
    "\n",
    "Temperatura = 1.5:\n",
    "\n",
    "Aumenta la creatividad, pero disminuye la coherencia.\n",
    "\n",
    "\n",
    "\n",
    "Impacto de Longitud de Secuencia\n",
    "\n",
    "Secuencia: 5 palabras\n",
    "\n",
    "El modelo tiene menos contexto para generar texto, lo que resulta en frases más desconectadas.\n",
    "\n",
    "\n",
    "Secuencia: 10 palabras\n",
    "\n",
    "Proporciona un equilibrio adecuado entre contexto y creatividad.\n",
    "\n",
    "\n",
    "Secuencia: 20 palabras\n",
    "\n",
    "Ofrece el máximo contexto, lo que mejora la coherencia y el flujo lógico.\n",
    "Puede limitar la creatividad, especialmente con temperatura baja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
