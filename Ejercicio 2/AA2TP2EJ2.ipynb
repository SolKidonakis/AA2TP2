{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "jzeutgwovQRD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la GPU si está disponible\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "gL8QBtNowT4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f81bTkDzuacn",
        "outputId": "0c98e04d-796d-4c0e-ddaa-af680830bf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 1115394 caracteres\n"
          ]
        }
      ],
      "source": [
        "# Descargar el dataset\n",
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        ")\n",
        "\n",
        "# Leer el contenido\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Longitud del texto: {len(text)} caracteres\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras líneas\n",
        "print(text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co-2VDbcuxBw",
        "outputId": "3bd854f4-0790-4ece-abee-00d51cfc4a46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} caracteres únicos en el texto')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOYJvYv_u7IO",
        "outputId": "64d24b60-7b72-4797-8842-cf1f957fdffb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39 caracteres únicos en el texto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELO CARACTER A CARACTER**"
      ],
      "metadata": {
        "id": "b6Ecfz7HwdC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear capas de mapeo de caracteres a IDs y viceversa. La capa tf.keras.layers.StringLookup nos permite convertir cada caracter en un ID numerico."
      ],
      "metadata": {
        "id": "7DBKgrVzwpGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(sorted(set(text))), mask_token=None)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n"
      ],
      "metadata": {
        "id": "oL7V1YKIwgKC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generar secuencias de entrenamiento"
      ],
      "metadata": {
        "id": "BRA1-0m3wuci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n"
      ],
      "metadata": {
        "id": "NIx4Cvr6wvE2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINICION DEL MODELO RNN CON GRU"
      ],
      "metadata": {
        "id": "FP1_80duyGUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False):\n",
        "        x = self.embedding(inputs)  # Aplicamos el embedding\n",
        "        if states is None:\n",
        "            states = tf.zeros((inputs.shape[0], self.gru.units))  # Generar estado inicial con tamaño correcto\n",
        "        x, states = self.gru(x, initial_state=states)  # Pasamos estado inicial correcto\n",
        "        x = self.dense(x)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "XewuWIfbyBbk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model_char = CharacterModel(vocab_size, embedding_dim, rnn_units)\n",
        "model_char.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
      ],
      "metadata": {
        "id": "GKKaltKryOC8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model_char.fit(dataset, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCFbMEjayRK7",
        "outputId": "f020f4f0-5f67-47fa-d52c-726d1def89aa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 5s/step - loss: 2.8926\n",
            "Epoch 2/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 5s/step - loss: 1.8444\n",
            "Epoch 3/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 5s/step - loss: 1.5629\n",
            "Epoch 4/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 5s/step - loss: 1.4319\n",
            "Epoch 5/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 5s/step - loss: 1.3572\n",
            "Epoch 6/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m914s\u001b[0m 5s/step - loss: 1.3026\n",
            "Epoch 7/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m923s\u001b[0m 5s/step - loss: 1.2550\n",
            "Epoch 8/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m926s\u001b[0m 5s/step - loss: 1.2078\n",
            "Epoch 9/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m925s\u001b[0m 5s/step - loss: 1.1705\n",
            "Epoch 10/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m932s\u001b[0m 5s/step - loss: 1.1262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de Texto**"
      ],
      "metadata": {
        "id": "XMQo_CWwSnkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :] / self.temperature\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "        return predicted_chars, states\n"
      ],
      "metadata": {
        "id": "0IINUquHSph4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_char = OneStep(model_char, chars_from_ids, ids_from_chars)\n",
        "states = None\n",
        "next_char = tf.constant([\"To be or not to be\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "    next_char, states = generator_char.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqFYB294S7ee",
        "outputId": "c4a1522a-3ccd-45c4-dbb6-0b0b8f86f0a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be private friend.\n",
            "\n",
            "isabella:\n",
            "o, that thou rightsing your days like men of that;\n",
            "for might be two never half.\n",
            "\n",
            "gloucester:\n",
            "what very thought offer'd to my daughter,\n",
            "and thus scope: and she'll say how learn, sir; 'puce,\n",
            "of any that with in exerticely?\n",
            "\n",
            "gloucester:\n",
            "\n",
            "buckingham:\n",
            "what, have i offenced wedcheard, green.\n",
            "\n",
            "lady capulet:\n",
            "ah, what my bodies life; and thush a mortal semm:\n",
            "yet behtlembing than my imparised are here. if my lord,\n",
            "what was how edward, and clifford,\n",
            "dispetion'd crast in a hillfi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELO PALABRA A PALABRA**"
      ],
      "metadata": {
        "id": "XGiilmZ-XWLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size_word = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences_word = tokenizer.texts_to_sequences([text])[0]\n"
      ],
      "metadata": {
        "id": "SBRfUiuXXauc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 20\n",
        "sequences = [\n",
        "    sequences_word[i:i + seq_length + 1] for i in range(len(sequences_word) - seq_length)\n",
        "]\n",
        "sequences = np.array(sequences)\n"
      ],
      "metadata": {
        "id": "Ximws9chXj5D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size_word)\n"
      ],
      "metadata": {
        "id": "IVcZfyNwXsk2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_word = Sequential([\n",
        "    Embedding(vocab_size_word, embedding_dim, input_length=seq_length),\n",
        "    LSTM(150, return_sequences=True),\n",
        "    LSTM(150),\n",
        "    Dense(vocab_size_word, activation='softmax')\n",
        "])\n",
        "\n",
        "model_word.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aD8TrSNX067",
        "outputId": "7a0c500c-b2a6-4ca6-ef04-94df63e6eda1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "model_word.fit(X, y, batch_size=64, epochs=EPOCHS)\n"
      ],
      "metadata": {
        "id": "_ENWFOBDYAsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_word(model, tokenizer, seed_text, num_words, temperature=1.0):\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = tf.expand_dims(token_list[-seq_length:], axis=0)\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "            tf.math.log(predictions), num_samples=1).numpy()[0, 0]\n",
        "\n",
        "        output_word = tokenizer.index_word.get(predicted_id, \"\")\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n"
      ],
      "metadata": {
        "id": "uU1L3Iy8Y2jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"To be or not to be\"\n",
        "print(generate_text_word(model_word, tokenizer, seed_text, num_words=50, temperature=1.0))\n"
      ],
      "metadata": {
        "id": "jXY1tXZaY6FS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}